{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T12:13:23.217758Z",
     "start_time": "2021-02-25T12:13:09.996658Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.3.3-py3-none-any.whl (1.9 MB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\dongh\\anaconda3\\lib\\site-packages (from transformers) (2020.10.15)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\dongh\\anaconda3\\lib\\site-packages (from transformers) (4.50.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\dongh\\anaconda3\\lib\\site-packages (from transformers) (3.0.12)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.43.tar.gz (883 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\dongh\\anaconda3\\lib\\site-packages (from transformers) (1.19.2)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.1-cp38-cp38-win_amd64.whl (2.0 MB)\n",
      "Requirement already satisfied: packaging in c:\\users\\dongh\\anaconda3\\lib\\site-packages (from transformers) (20.4)\n",
      "Requirement already satisfied: requests in c:\\users\\dongh\\anaconda3\\lib\\site-packages (from transformers) (2.24.0)\n",
      "Requirement already satisfied: six in c:\\users\\dongh\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in c:\\users\\dongh\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\dongh\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (0.17.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\dongh\\anaconda3\\lib\\site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dongh\\anaconda3\\lib\\site-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\dongh\\anaconda3\\lib\\site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\dongh\\anaconda3\\lib\\site-packages (from requests->transformers) (1.25.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\dongh\\anaconda3\\lib\\site-packages (from requests->transformers) (2.10)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py): started\n",
      "  Building wheel for sacremoses (setup.py): finished with status 'done'\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.43-py3-none-any.whl size=893262 sha256=8db64d7367ee61b99d7c82e7f05a5f2219e6376a8e4489eabf9bb19c12000a4d\n",
      "  Stored in directory: c:\\users\\dongh\\appdata\\local\\pip\\cache\\wheels\\7b\\78\\f4\\27d43a65043e1b75dbddaa421b573eddc67e712be4b1c80677\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: sacremoses, tokenizers, transformers\n",
      "Successfully installed sacremoses-0.0.43 tokenizers-0.10.1 transformers-4.3.3\n"
     ]
    }
   ],
   "source": [
    "# HuggingFace transformers 설치 및 NSMC 데이터셋 다운로드\n",
    "!pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-02-25T12:13:09.999Z"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install torch==1.7.0\n",
    "!pip install torch===1.7.1 torchvision===0.8.2 torchaudio===0.7.2 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-02-25T12:13:10.001Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, ElectraForSequenceClassification, AdamW\n",
    "from tqdm.notebook import tqdm\n",
    "import gc\n",
    "from sklearn.model_selection import KFold\n",
    "import wget\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-02-25T12:13:10.002Z"
    }
   },
   "outputs": [],
   "source": [
    "wget.download('https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt')\n",
    "wget.download('https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt')\n",
    "wget.download('https://raw.githubusercontent.com/donghyun-daniel/4_party_project/master/Review_crawler/csv_output/Preprocessing_review_pos_neg.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-02-25T12:13:10.003Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-02-25T12:13:10.004Z"
    }
   },
   "outputs": [],
   "source": [
    "class NSMCDataset(Dataset):\n",
    "  \n",
    "    def __init__(self, csv_file):\n",
    "        # 일부 값중에 NaN이 있음...\n",
    "        self.dataset = pd.read_csv(csv_file, sep='\\t').dropna(axis=0) \n",
    "        # 중복제거\n",
    "        self.dataset.drop_duplicates(subset=['document'], inplace=True)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"monologg/koelectra-small-v2-discriminator\")\n",
    "\n",
    "        print(self.dataset.describe())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataset.iloc[idx, 1:3].values\n",
    "        text = row[0]\n",
    "        y = row[1]\n",
    "\n",
    "        inputs = self.tokenizer(\n",
    "            text, \n",
    "            return_tensors='pt',\n",
    "            truncation=True,\n",
    "            max_length=256,\n",
    "            pad_to_max_length=True,\n",
    "            add_special_tokens=True\n",
    "            )\n",
    "\n",
    "        input_ids = inputs['input_ids'][0]\n",
    "        attention_mask = inputs['attention_mask'][0]\n",
    "\n",
    "        return input_ids, attention_mask, y\n",
    "\n",
    "\n",
    "    \n",
    "def data_concat(naverTrainData, naverTestData, hotelData): \n",
    "    #data_concat(\"ratings_train.txt\", \"ratings_test.txt\", \"Preprocessing_review_pos_neg.csv\")\n",
    "    trainRate = 0.5\n",
    "    \n",
    "    data1 = pd.read_table(naverTrainData)\n",
    "    data2 = pd.read_table(naverTestData)\n",
    "    data3 = pd.read_csv(hotelData)\n",
    "    \n",
    "    data3 = data3.drop(['len_text'], axis=1)\n",
    "    data3['id'] = 1\n",
    "    data3 = data3[['id', 'Text', 'Label']]\n",
    "    data3.columns = ['id','document','label']\n",
    "    data3 = data3.iloc[np.random.permutation(data3.index)].reset_index(drop=True)\n",
    "    \n",
    "    data3Train = data3.sample(frac = trainRate, random_state=2000)\n",
    "    data3Test = data3.drop(data3Train.index)\n",
    "    \n",
    "    trainData = pd.concat([data1, data3Train], ignore_index=True)\n",
    "    testData = pd.concat([data2, data3Test], ignore_index=True)\n",
    "    \n",
    "    trainData.to_csv('sampleTrain.txt', sep = '\\t', index = False)\n",
    "    testData.to_csv('sampleTest.txt', sep = '\\t', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-02-25T12:13:10.006Z"
    }
   },
   "outputs": [],
   "source": [
    "data_concat(\"ratings_train.txt\", \"ratings_test.txt\", \"Preprocessing_review_pos_neg.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-02-25T12:13:10.007Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = NSMCDataset(\"sampleTrain.txt\")\n",
    "test_dataset = NSMCDataset(\"sampleTest.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-02-25T12:13:10.008Z"
    }
   },
   "outputs": [],
   "source": [
    "model = ElectraForSequenceClassification.from_pretrained(\"monologg/koelectra-small-v2-discriminator\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-02-25T12:13:10.009Z"
    }
   },
   "outputs": [],
   "source": [
    "# 에러나도 그냥 넘어가면 됨\n",
    "model.load_state_dict(torch.load(\"model.pt\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-02-25T12:13:10.010Z"
    }
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-02-25T12:13:10.011Z"
    }
   },
   "outputs": [],
   "source": [
    "# GPU 메모리 터지는거 방지\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-02-25T12:13:10.012Z"
    }
   },
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "batch_size = 64\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "# 10 fold cross val\n",
    "kfold = KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "kfold_losses = []\n",
    "kfold_acc = []\n",
    "\n",
    "\n",
    "for fold, (train_ids, val_ids) in enumerate(kfold.split(train_dataset)):\n",
    "    print('----------------------------------')\n",
    "    print(fold)\n",
    "\n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "    val_subsampler = torch.utils.data.SubsetRandomSampler(val_ids)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_subsampler)\n",
    "    val_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=val_subsampler)\n",
    "    \n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        batches = 0\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for input_ids_batch, attention_masks_batch, y_batch in tqdm(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            y_batch = y_batch.to(device)\n",
    "            y_pred = model(input_ids_batch.to(device), attention_mask=attention_masks_batch.to(device))[0]\n",
    "            loss = F.cross_entropy(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(y_pred, 1)\n",
    "            correct += (predicted == y_batch).sum()\n",
    "            total += len(y_batch)\n",
    "\n",
    "            batches += 1\n",
    "            if batches % 100 == 0:\n",
    "                print(\"Batch Loss:\", total_loss, \"Accuracy:\", correct.float() / total)\n",
    "\n",
    "        losses.append(total_loss)\n",
    "        accuracies.append(correct.float() / total)\n",
    "        print(\"Train Loss:\", total_loss, \"Accuracy:\", correct.float() / total)\n",
    "    loss_avg = sum(losses) / len(losses)\n",
    "    acc_avg = sum(accuracies) / len(accuracies)\n",
    "    kfold_losses.append(loss_avg)\n",
    "    kfold_acc.append(acc_avg)\n",
    "    \n",
    "    print('----------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-02-25T12:13:10.013Z"
    }
   },
   "outputs": [],
   "source": [
    "kfold_losses, kfold_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-02-25T12:13:10.014Z"
    }
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "for input_ids_batch, attention_masks_batch, y_batch in tqdm(test_loader):\n",
    "    y_batch = y_batch.to(device)\n",
    "    y_pred = model(input_ids_batch.to(device), attention_mask=attention_masks_batch.to(device))[0]\n",
    "    _, predicted = torch.max(y_pred, 1)\n",
    "    test_correct += (predicted == y_batch).sum()\n",
    "    test_total += len(y_batch)\n",
    "\n",
    "print(\"Accuracy:\", test_correct.float() / test_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-02-25T12:13:10.015Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pt\")\n",
    "\n",
    "## 여기까지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-02-25T12:13:10.016Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-02-25T12:13:10.017Z"
    }
   },
   "outputs": [],
   "source": [
    "_, pre=torch.max(y_pred, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-02-25T12:13:10.018Z"
    }
   },
   "outputs": [],
   "source": [
    "pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-02-25T12:13:10.019Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-02-25T12:13:10.020Z"
    }
   },
   "outputs": [],
   "source": [
    "train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-02-25T12:13:10.020Z"
    }
   },
   "outputs": [],
   "source": [
    "input_ids_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-02-25T12:13:10.021Z"
    }
   },
   "outputs": [],
   "source": [
    "attention_masks_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-02-25T12:13:10.022Z"
    }
   },
   "outputs": [],
   "source": [
    "y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-02-25T12:13:10.023Z"
    }
   },
   "outputs": [],
   "source": [
    "for input_ids_batch, attention_masks_batch, y_batch in tqdm(train_loader):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-02-25T12:13:10.024Z"
    }
   },
   "outputs": [],
   "source": [
    "input_ids_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-02-25T12:13:10.027Z"
    }
   },
   "outputs": [],
   "source": [
    "attention_masks_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-02-25T12:13:10.028Z"
    }
   },
   "outputs": [],
   "source": [
    "y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 올릴 방법\n",
    "\n",
    "- 어근 치환 ***\n",
    "- k-fold crossval \n",
    "\n",
    "- pretrain 모델 변경 ***\n",
    "\n",
    "- fine tuning\n",
    "// - 데이터 추가 영화, 호텔, 쇼핑 데이터\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-02-25T12:13:10.029Z"
    }
   },
   "outputs": [],
   "source": [
    "data = [1,2,3,4,5,6,7,8,9,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
